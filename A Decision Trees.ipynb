{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322]() Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/) |\n",
    "[Sophina Luitel](https://www.gonzaga.edu/school-of-engineering-applied-science/faculty/detail/sophina-luitel-phd-0dba6a9d)\n",
    "\n",
    "---\n",
    "\n",
    "# Decision Trees\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn decision tree terminology\n",
    "* Introduce the TDIDT algorithm and clashes\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Gina Sprint's Data Science Algorithms notes\n",
    "* [Data Science from Scratch](https://jcer.in/jcer-docs/E-Learning/Digital%20Library%20/E-Books/Data%20Science%20from%20Scratch%20by%20Joel%20Grus.pdf) by Joel Grus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today 10/31\n",
    "* Announcements\n",
    "    * Happy Halloween!\n",
    "    * Project Overview and Requirements published on Canvas. Please review it and we will discuss it on Monday.\n",
    "    * PA5 is due on Tuesday.  \n",
    "        Note: Python is pass by object reference (this is important for train_test_split())\n",
    "    * PA6 will be posted later today.  \n",
    "    \n",
    "* Today\n",
    "    * LA9 notecard/quiz (15 mins)\n",
    "    * Remaining paper review presentation \n",
    "    * Go over Midterm questions\n",
    "    * Go over LA7 solution\n",
    "    * Classifer Performance Lab Task 2\n",
    "    * Intro to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Decision Tree Classifiers\n",
    "\n",
    "- KNN and Naive Bayes are instance-at-a-time classifiers:  \n",
    "  - Given a new instance, they use training set to predict class label \n",
    "  - KNN uses the specific training instances to predict and Naive Bayes uses the learned probabilities.  \n",
    "  - In both cases, itâ€™s hard to see why a particular prediction was made.\n",
    "  - Predictions are highly dependent on the specific instance (its attribute values). \n",
    "\n",
    "- Decision Trees are rule-based classifiers:  \n",
    "  - Build a set of general rules from the training data.  \n",
    "  - Predictions are made using the rules, not the raw training instances.  \n",
    "  - It is highly interpretable, you can see exactly why a prediction was made by following the tree from root to leaf.\n",
    "\n",
    "Rules are basic if-then statements:\n",
    "\n",
    ">IF $att_1 = val_1^1 \\wedge att_2 = val_1^2 \\wedge ...$ THEN $class = label_1$\n",
    "\n",
    ">IF $att_1 = val_2^1 \\wedge att_2 = val_2^2 \\wedge ...$ THEN $class = label_2$\n",
    "\n",
    ">IF $att_1 = val_3^1 \\wedge att_2 = val_3^2 \\wedge ...$ THEN $class = label_3$\n",
    "\n",
    "The rules are captured in a \"decision tree\"\n",
    "* Internal nodes denote attributes (e.g., job status, standing, etc.)\n",
    "* Edges denote values of the attribute\n",
    "* Leaves denote class labels (e.g., buys iphone = yes)\n",
    "    * Either stating a prediction\n",
    "    * Or giving the distribution...\n",
    "    \n",
    "### Lab Task 1\n",
    "An example for the iphone prediction example. iPhone Purchases (Fake) dataset:\n",
    "\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "A *clash* is when two or more instances in a partition have the same combination of attribute values but different classifications. \n",
    "\n",
    "Bramer's definition of the Top-Down Induction of Decision Trees (TDIDT) assumes the *adequacy condition*, which ensures that no two instances with identical attribute values have different class labels (e.g. no clashes).\n",
    "\n",
    "Does the iPhone dataset have any clashes?\n",
    "\n",
    "\n",
    "### Lab Task 2\n",
    "Here is an example decision tree for the iPhone dataset:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DataScienceAlgorithms/M5_DecisionTrees/main/figures/iphone_decision_tree_example.png\" width=\"850\"/>\n",
    "\n",
    "* Attribute values are edges/links that represent \"partitions\" of the training set\n",
    "* Leaf nodes give distribution of class labels\n",
    "\n",
    "Extract the rules from this decision tree.\n",
    "\n",
    "### Lab Task 3\n",
    "Use the tree from the previous task to classify the following test instances:\n",
    "1. $X_{1}$ = `[standing = 2, job_status = 2, credit_rating = fair]`\n",
    "1. $X_{2}$ = `[standing = 1, job_status = 1, credit_rating = excellent]`\n",
    "\n",
    "How do these predictions compare to the predicted class labels from LA7?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
