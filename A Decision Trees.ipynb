{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 322]() Data Science Algorithms\n",
    "[Gonzaga University](https://www.gonzaga.edu/) |\n",
    "[Sophina Luitel](https://www.gonzaga.edu/school-of-engineering-applied-science/faculty/detail/sophina-luitel-phd-0dba6a9d)\n",
    "\n",
    "---\n",
    "\n",
    "# Decision Trees\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn decision tree terminology\n",
    "* Introduce the TDIDT algorithm and clashes\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Gina Sprint's Data Science Algorithms notes\n",
    "* [Data Science from Scratch](https://jcer.in/jcer-docs/E-Learning/Digital%20Library%20/E-Books/Data%20Science%20from%20Scratch%20by%20Joel%20Grus.pdf) by Joel Grus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today 10/31\n",
    "* Announcements\n",
    "    * Happy Halloween!\n",
    "    * Project Overview and Requirements published on Canvas. Please review it and we will discuss it on Monday.\n",
    "    * PA5 is due on Tuesday.  \n",
    "        Note: Python is pass by object reference (this is important for train_test_split())\n",
    "    * PA6 will be posted later today.  \n",
    "    \n",
    "* Today \n",
    "    * LA9 notecard/quiz (15 mins)\n",
    "    * Remaining paper review presentation \n",
    "    * Go over Midterm questions\n",
    "    * Go over LA7 solution\n",
    "    * Classifer Performance Lab Task 2\n",
    "    * Intro to Decision Trees  \n",
    "##  Today 11/03 \n",
    "\n",
    "* Classifer Performance Lab Task 2\n",
    "* Intro to Decision Trees\n",
    "* TDIDT algorithm and clashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Decision Tree Classifiers\n",
    "\n",
    "- KNN and Naive Bayes are instance-at-a-time classifiers:  \n",
    "  - Given a new instance, they use training set to predict class label \n",
    "  - KNN uses the specific training instances to predict and Naive Bayes uses the learned probabilities.  \n",
    "  - In both cases, it’s hard to see why a particular prediction was made.\n",
    "  - Predictions are highly dependent on the specific instance (its attribute values). \n",
    "\n",
    "- Decision Trees are rule-based classifiers:  \n",
    "  - Build a set of general rules from the training data.  \n",
    "  - Predictions are made using the rules, not the raw training instances.  \n",
    "  - It is highly interpretable, you can see exactly why a prediction was made by following the tree from root to leaf.\n",
    "\n",
    "Rules are basic if-then statements:\n",
    "\n",
    ">IF $att_1 = val_1^1 \\wedge att_2 = val_1^2 \\wedge ...$ THEN $class = label_1$\n",
    "\n",
    ">IF $att_1 = val_2^1 \\wedge att_2 = val_2^2 \\wedge ...$ THEN $class = label_2$\n",
    "\n",
    ">IF $att_1 = val_3^1 \\wedge att_2 = val_3^2 \\wedge ...$ THEN $class = label_3$\n",
    "\n",
    "The rules are captured in a \"decision tree\"\n",
    "* Internal nodes denote attributes (e.g., job status, standing, etc.)\n",
    "* Edges denote values of the attribute\n",
    "* Leaves denote class labels (e.g., buys iphone = yes)\n",
    "    * Either stating a prediction\n",
    "    * Or giving the distribution...\n",
    "    \n",
    "### Lab Task 1\n",
    "An example for the iphone prediction example. iPhone Purchases (Fake) dataset:\n",
    "\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "A *clash* is when two or more instances in a partition have the same combination of attribute values but different classifications. \n",
    "\n",
    "Bramer's definition of the Top-Down Induction of Decision Trees (TDIDT) assumes the *adequacy condition*, which ensures that no two instances with identical attribute values have different class labels (e.g. no clashes).\n",
    "\n",
    "Does the iPhone dataset have any clashes?\n",
    "\n",
    "\n",
    "### Lab Task 2\n",
    "Here is an example decision tree for the iPhone dataset:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DataScienceAlgorithms/M5_DecisionTrees/main/figures/iphone_decision_tree_example.png\" width=\"850\"/>\n",
    "\n",
    "* Attribute values are edges/links that represent \"partitions\" of the training set\n",
    "* Leaf nodes give distribution of class labels\n",
    "\n",
    "Extract the rules from this decision tree.\n",
    "\n",
    "### Lab Task 3\n",
    "Use the tree from the previous task to classify the following test instances:\n",
    "1. $X_{1}$ = `[standing = 2, job_status = 2, credit_rating = fair]`\n",
    "1. $X_{2}$ = `[standing = 1, job_status = 1, credit_rating = excellent]`\n",
    "\n",
    "How do these predictions compare to the predicted class labels from LA7?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TDIDT (Top-Down Induction of Decision Trees) Algorithm\n",
    "Basic Approach (uses recursion!):\n",
    "* At each step, pick an attribute (\"attribute selection\")\n",
    "* Partition data by attribute values ... this creates pairwise disjoint partitions\n",
    "* Repeat until one of the following occurs (base cases):\n",
    "    1. Partition has only class labels that are the same ... no clashes, make a leaf node\n",
    "    2. No more attributes to partition ... reached the end of a branch and there may be clashes, see options below\n",
    "    3. No more instances to partition ... see options below\n",
    "    \n",
    "### More on Case 3\n",
    "Assume we have the following:\n",
    "<img src=\"https://raw.githubusercontent.com/DataScienceAlgorithms/M5_DecisionTrees/main/figures/decision_tree_one_attr.png\" width=\"300\"/>\n",
    "\n",
    "* Where the partition for att1=v1 has many instances\n",
    "* But the partition for att1=v2 has no instances\n",
    "* What are our options?\n",
    "    1. Do Nothing: Leave value out of tree (creates incomplete decision tree)\n",
    "    2. Backtrack: replace Attribute 1 node with leaf node (possibly w/clashes, see options below)\n",
    "* For the first choice, we won't be able to classify all instances\n",
    "* We also need to know the possible attribute values ahead of time\n",
    "\n",
    "### Handling Clashes for Prediction\n",
    "1. \"Majority Voting\"... select the class with highest number of instances\n",
    "    * On ties, \"flip a coin\"... which for ease of reproducibility could simply be choosing the first label alphabetically\n",
    "2. \"Intuition\"... that is, use common sense and pick one (hand modify tree)\n",
    "3. \"Discard\"... remove the branch from the node above\n",
    "    * Similar to case 3 above\n",
    "    * Results in \"missing\" attribute combos (some instances can't be classified)\n",
    "    * e.g., just remove two 50/50 branches from iPhone example tree\n",
    "    \n",
    "### Summary: TDIDT Algorithm (w/backtracking and majority voting)\n",
    "1. At each step, select an attribute to split on (“attribute selection” e.g. random, takefirst, takelast, entropy, gini, etc.)\n",
    "1. Group the data by attribute domain... (e.g. create pairwise disjoint partitions)\n",
    "1. For each partition, repeat unless one of the following occurs (base cases):\n",
    "    1. CASE 1: All class labels of the partition are the same (e.g. no clashes)\n",
    "        * => create a leaf node\n",
    "    1. CASE 2: No more attributes to split on (e.g. clash)\n",
    "        * => handle the clash with a majority vote leaf node\n",
    "    1. CASE 3: No more instances to partition (e.g. empty partition)\n",
    "        * => backtrack and replace subtree with majority vote leaf node\n",
    "\n",
    "Note: On majority vote ties, choose first label alphabetically (simplest for reproducibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task 4\n",
    "Use TDIDT to create a decision tree for iPhone example. Randomly select attributes as your \"attribute selection\" approach.\n",
    "\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "### Lab Task 5\n",
    "Using the tree from the previous step, predict the class labels again for the following test instances:\n",
    "1. $X_{1}$ = `[standing = 2, job_status = 2, credit_rating = fair]`\n",
    "1. $X_{2}$ = `[standing = 1, job_status = 1, credit_rating = excellent]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
